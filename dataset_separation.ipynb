{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ac3e29",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2515a38f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mВнимание: нет аннотации для \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Копируем файлы\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mcopy_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_images_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m copy_files(val_images, val_images_dir, val_labels_dir)\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m изображений\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mcopy_files\u001b[39m\u001b[34m(file_list, images_dst, labels_dst)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m file_list:\n\u001b[32m     40\u001b[39m     label_path = labels_dir / (img_path.stem + \u001b[33m\"\u001b[39m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_dst\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m label_path.exists():\n\u001b[32m     44\u001b[39m         shutil.copy(label_path, labels_dst / label_path.name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gusata\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\shutil.py:428\u001b[39m, in \u001b[36mcopy\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(dst):\n\u001b[32m    427\u001b[39m     dst = os.path.join(dst, os.path.basename(src))\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m copymode(src, dst, follow_symlinks=follow_symlinks)\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gusata\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\shutil.py:260\u001b[39m, in \u001b[36mcopyfile\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    258\u001b[39m     os.symlink(os.readlink(src), dst)\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[32m    261\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    262\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[32m    263\u001b[39m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "# Один раз загружаем словарь из файла\n",
    "with open('item_id_map.json', 'r', encoding='utf-8') as f:\n",
    "    name_to_id = json.load(f)\n",
    "\n",
    "# Параметры\n",
    "dataset_dir = Path(\"dataset\")\n",
    "images_dir = dataset_dir / \"images\" / \"train_full\"\n",
    "labels_dir = dataset_dir / \"labels\" / \"train_full\"\n",
    "\n",
    "train_images_dir = dataset_dir / \"images\" / \"train\"\n",
    "val_images_dir = dataset_dir / \"images\" / \"val\"\n",
    "train_labels_dir = dataset_dir / \"labels\" / \"train\"\n",
    "val_labels_dir = dataset_dir / \"labels\" / \"val\"\n",
    "\n",
    "val_ratio = 0.2  # 20% для валидации\n",
    "\n",
    "# Создаём папки, если их нет\n",
    "for d in [train_images_dir, val_images_dir, train_labels_dir, val_labels_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Получаем список всех файлов изображений\n",
    "all_images = sorted(images_dir.glob(\"*.png\"))  # подстрой под формат, если другой\n",
    "\n",
    "# Перемешиваем для случайного разбиения\n",
    "random.shuffle(all_images)\n",
    "\n",
    "num_val = int(len(all_images) * val_ratio)\n",
    "val_images = all_images[:num_val]\n",
    "train_images = all_images[num_val:]\n",
    "\n",
    "def copy_files(file_list, images_dst, labels_dst):\n",
    "    for img_path in file_list:\n",
    "        label_path = labels_dir / (img_path.stem + \".txt\")\n",
    "\n",
    "        shutil.copy(img_path, images_dst / img_path.name)\n",
    "        if label_path.exists():\n",
    "            shutil.copy(label_path, labels_dst / label_path.name)\n",
    "        else:\n",
    "            print(f\"Внимание: нет аннотации для {img_path.name}\")\n",
    "\n",
    "# Копируем файлы\n",
    "copy_files(train_images, train_images_dir, train_labels_dir)\n",
    "copy_files(val_images, val_images_dir, val_labels_dir)\n",
    "\n",
    "print(f\"Train: {len(train_images)} изображений\")\n",
    "print(f\"Val: {len(val_images)} изображений\")\n",
    "\n",
    "# Генерация data.yaml для YOLOv8\n",
    "data_yaml = {\n",
    "    'path': str(dataset_dir.resolve()),  # корневая папка dataset\n",
    "    'train': str(train_images_dir.relative_to(dataset_dir)),\n",
    "    'val': str(val_images_dir.relative_to(dataset_dir)),\n",
    "    'nc': len(name_to_id),  # количество классов, у тебя должно быть определено name_to_id\n",
    "    'names': [None] * len(name_to_id)\n",
    "}\n",
    "\n",
    "# Заполним имена классов в порядке id (предполагается, что name_to_id — dict {name: id})\n",
    "for name, idx in name_to_id.items():\n",
    "    data_yaml['names'][idx] = name\n",
    "\n",
    "yaml_path = dataset_dir / \"data.yaml\"\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    yaml.dump(data_yaml, f, sort_keys=False)\n",
    "\n",
    "print(f\"Сгенерирован файл {yaml_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8105728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 8000/8000 [06:32<00:00, 20.40it/s]\n",
      "Val: 100%|██████████| 2000/2000 [01:43<00:00, 19.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8000 изображений\n",
      "Val: 2000 изображений\n",
      "Сгенерирован файл dataset\\data.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "from concurrent.futures import ThreadPoolExecutor  # вместо ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Один раз загружаем словарь из файла\n",
    "with open('item_id_map.json', 'r', encoding='utf-8') as f:\n",
    "    name_to_id = json.load(f)\n",
    "\n",
    "# Параметры\n",
    "dataset_dir = Path(\"dataset\")\n",
    "images_dir = dataset_dir / \"images\" / \"train_full\"\n",
    "labels_dir = dataset_dir / \"labels\" / \"train_full\"\n",
    "\n",
    "train_images_dir = dataset_dir / \"images\" / \"train\"\n",
    "val_images_dir = dataset_dir / \"images\" / \"val\"\n",
    "train_labels_dir = dataset_dir / \"labels\" / \"train\"\n",
    "val_labels_dir = dataset_dir / \"labels\" / \"val\"\n",
    "\n",
    "val_ratio = 0.2  # 20% для валидации\n",
    "\n",
    "# Создаём папки, если их нет\n",
    "for d in [train_images_dir, val_images_dir, train_labels_dir, val_labels_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Получаем список всех файлов изображений\n",
    "all_images = sorted(images_dir.glob(\"*.png\"))\n",
    "random.shuffle(all_images)\n",
    "\n",
    "num_val = int(len(all_images) * val_ratio)\n",
    "val_images = all_images[:num_val]\n",
    "train_images = all_images[num_val:]\n",
    "\n",
    "\n",
    "\n",
    "def copy_pair(img_path_str, images_dst_str, labels_dst_str):\n",
    "    try:\n",
    "        img_path = Path(img_path_str)\n",
    "        images_dst = Path(images_dst_str)\n",
    "        labels_dst = Path(labels_dst_str)\n",
    "\n",
    "        label_path = labels_dir / (img_path.stem + \".txt\")\n",
    "        shutil.copy(img_path, images_dst / img_path.name)\n",
    "        if label_path.exists():\n",
    "            shutil.copy(label_path, labels_dst / label_path.name)\n",
    "            return f\"OK: {img_path.name}\"\n",
    "        else:\n",
    "            return f\"Нет аннотации: {img_path.name}\"\n",
    "    except Exception as e:\n",
    "        return f\"Ошибка: {img_path.name} — {e}\"\n",
    "\n",
    "def copy_files_parallel(file_list, images_dst, labels_dst, desc=\"Копирование\"):\n",
    "    images_dst_str = str(images_dst)\n",
    "    labels_dst_str = str(labels_dst)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(copy_pair, str(img), images_dst_str, labels_dst_str)\n",
    "            for img in file_list\n",
    "        ]\n",
    "        for f in tqdm(futures, desc=desc):\n",
    "            msg = f.result()\n",
    "            if not msg.startswith(\"OK\"):\n",
    "                print(msg)\n",
    "\n",
    "# Копируем с прогрессбаром и многопроцессорностью\n",
    "copy_files_parallel(train_images, train_images_dir, train_labels_dir, desc=\"Train\")\n",
    "copy_files_parallel(val_images, val_images_dir, val_labels_dir, desc=\"Val\")\n",
    "\n",
    "print(f\"Train: {len(train_images)} изображений\")\n",
    "print(f\"Val: {len(val_images)} изображений\")\n",
    "\n",
    "# Генерация data.yaml\n",
    "data_yaml = {\n",
    "    'path': str(dataset_dir.resolve()),\n",
    "    'train': str(train_images_dir.relative_to(dataset_dir)),\n",
    "    'val': str(val_images_dir.relative_to(dataset_dir)),\n",
    "    'nc': len(name_to_id),\n",
    "    'names': [None] * len(name_to_id)\n",
    "}\n",
    "for name, idx in name_to_id.items():\n",
    "    data_yaml['names'][idx] = name\n",
    "\n",
    "yaml_path = dataset_dir / \"data.yaml\"\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    yaml.dump(data_yaml, f, sort_keys=False)\n",
    "\n",
    "print(f\"Сгенерирован файл {yaml_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
